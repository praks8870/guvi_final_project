{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER SENTIMENT ANALYSIS\n",
    "\n",
    "The following project is about analyzing the sentiments of tweets on social networking website\n",
    "‘Twitter’. The dataset for this project is scraped from Twitter. It contains 1,600,000 tweets\n",
    "extracted using Twitter API. It is a labeled dataset with tweets annotated with the sentiment (0 =\n",
    "negative, 2 = neutral, 4 = positive).\n",
    "It contains the following 6 fields:\n",
    "\n",
    "1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "2. ids: The id of the tweet .\n",
    "3. date: The date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: The query. If there is no query, then this value is NO_QUERY.\n",
    "5. user: The user that tweeted\n",
    "6. text: The text of the tweet.\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "In today's digital age, social media platforms like Twitter have become a crucial medium for individuals to express their opinions and sentiments. For businesses, understanding the sentiment of Twitter users towards their brand is essential for maintaining a positive brand image and making informed marketing decisions. However, manually analyzing thousands of tweets to gauge sentiment is time-consuming and prone to human error. Therefore, there is a need for an automated system that can accurately analyze the sentiment of tweets related to a brand and provide actionable insights for brand management and marketing strategies. Design a classification model that correctly predicts the polarity of the tweets provided in the\n",
    "dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,precision_score,f1_score,roc_auc_score,roc_curve\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import nltk\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         status          id                          date     query  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    name                                              tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['status', 'id', 'date', 'query', 'name', 'tweet']\n",
    "\n",
    "data = pd.read_csv(r\"D:\\datascience\\twitter_guvi_project\\twitter_new.csv\", names = column_names, encoding='latin1')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status     int64\n",
       "id         int64\n",
       "date      object\n",
       "query     object\n",
       "name      object\n",
       "tweet     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         status                                              tweet\n",
       "0             0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1             0  is upset that he can't update his Facebook by ...\n",
       "2             0  @Kenichan I dived many times for the ball. Man...\n",
       "3             0    my whole body feels itchy and like its on fire \n",
       "4             0  @nationwideclass no, it's not behaving at all....\n",
       "...         ...                                                ...\n",
       "1599995       4  Just woke up. Having no school is the best fee...\n",
       "1599996       4  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997       4  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998       4  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999       4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[['status', 'tweet']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['status'].replace(4, 1, inplace = True)\n",
    "df1['tweet'] = df1['tweet'].str.lower()\n",
    "df1['status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select A minimum Number of data from the huge dataset for Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0</td>\n",
       "      <td>@heather2711 good thing i didn't find any then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>0</td>\n",
       "      <td>dea's are no fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>0</td>\n",
       "      <td>@tommcfly no i haven't  hey but you guys are b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>0</td>\n",
       "      <td>i googled up remedies for jonzing and reading ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>0</td>\n",
       "      <td>trying to fix my phone         fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839995</th>\n",
       "      <td>1</td>\n",
       "      <td>oficially done with drivers ed stuff. now i ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839996</th>\n",
       "      <td>1</td>\n",
       "      <td>@jimhunt thank you for sharing and caring!  gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839997</th>\n",
       "      <td>1</td>\n",
       "      <td>@buberzionist ok objection sustained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839998</th>\n",
       "      <td>1</td>\n",
       "      <td>is twittin' while my baby girl sleeps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839999</th>\n",
       "      <td>1</td>\n",
       "      <td>but with facebook u can share loads more too.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        status                                              tweet\n",
       "20000        0  @heather2711 good thing i didn't find any then...\n",
       "20001        0                                 dea's are no fun  \n",
       "20002        0  @tommcfly no i haven't  hey but you guys are b...\n",
       "20003        0  i googled up remedies for jonzing and reading ...\n",
       "20004        0                trying to fix my phone         fail\n",
       "...        ...                                                ...\n",
       "839995       1  oficially done with drivers ed stuff. now i ju...\n",
       "839996       1  @jimhunt thank you for sharing and caring!  gl...\n",
       "839997       1              @buberzionist ok objection sustained \n",
       "839998       1             is twittin' while my baby girl sleeps \n",
       "839999       1  but with facebook u can share loads more too.....\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_p = df1[df1['status'] == 1]\n",
    "df1_n = df1[df1['status'] == 0]\n",
    "\n",
    "df1p = df1_p.iloc[:int(20000)]\n",
    "df1p2 = df1_p.iloc[20000:40000]\n",
    "\n",
    "df1n = df1_n.iloc[:int(20000)]\n",
    "df1n2 = df1_n.iloc[20000:40000]\n",
    "\n",
    "df2 = pd.concat([df1n2, df1p2])\n",
    "# df2 = pd.concat([df1n, df1p])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "', '.join(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the tweet and removing the special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweet = \" \".join([word for word in str(tweet).split() if word not in stop_words])\n",
    "\n",
    "    eng_punctuations = string.punctuation\n",
    "    translator =  str.maketrans('', '', eng_punctuations)\n",
    "\n",
    "    tweet = tweet.translate(translator)\n",
    "\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "\n",
    "    tweet = re.sub('@[^\\s]+', ' ', tweet)\n",
    "\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ', tweet)\n",
    "\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0</td>\n",
       "      <td>heather god thing find none ones like come siz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>0</td>\n",
       "      <td>deas fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>0</td>\n",
       "      <td>tomcfly hey guys back england wel super great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>0</td>\n",
       "      <td>gogled remedies jonzing reading one sugestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>0</td>\n",
       "      <td>trying fix phone fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839995</th>\n",
       "      <td>1</td>\n",
       "      <td>oficialy done drivers ed stuf wait june licens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839996</th>\n",
       "      <td>1</td>\n",
       "      <td>jimhunt thank sharing caring glad like quotes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839997</th>\n",
       "      <td>1</td>\n",
       "      <td>buberzionist ok objection sustained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839998</th>\n",
       "      <td>1</td>\n",
       "      <td>twitin baby girl sleps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839999</th>\n",
       "      <td>1</td>\n",
       "      <td>facebok u share loads to myspace se promotiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        status                                              tweet\n",
       "20000        0  heather god thing find none ones like come siz...\n",
       "20001        0                                           deas fun\n",
       "20002        0  tomcfly hey guys back england wel super great ...\n",
       "20003        0  gogled remedies jonzing reading one sugestions...\n",
       "20004        0                              trying fix phone fail\n",
       "...        ...                                                ...\n",
       "839995       1  oficialy done drivers ed stuf wait june licens...\n",
       "839996       1      jimhunt thank sharing caring glad like quotes\n",
       "839997       1                buberzionist ok objection sustained\n",
       "839998       1                             twitin baby girl sleps\n",
       "839999       1  facebok u share loads to myspace se promotiona...\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['tweet'] = df2['tweet'].apply(lambda x: clean_text(x))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets has to be lemmatized and tokenized for the model creation\n",
    "\n",
    "Lemmatization: Thiis is a process of getting a root of the word. In example the words 'leave' and 'leaves' both word are comming from the root word of  'leaf', lemmatizing is the process of getting the root of the word. Tokenization is the process of deviding the sentences into tokens (single word) to analyse the word and use them for the model.\n",
    "\n",
    "In the following proces Iam spliting the words into tokens then lemmatizing the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0</td>\n",
       "      <td>heather god thing find none one like come size...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>0</td>\n",
       "      <td>dea fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>0</td>\n",
       "      <td>tomcfly hey guy back england wel super great l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>0</td>\n",
       "      <td>gogled remedy jonzing reading one sugestions t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>0</td>\n",
       "      <td>trying fix phone fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839995</th>\n",
       "      <td>1</td>\n",
       "      <td>oficialy done driver ed stuf wait june license...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839996</th>\n",
       "      <td>1</td>\n",
       "      <td>jimhunt thank sharing caring glad like quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839997</th>\n",
       "      <td>1</td>\n",
       "      <td>buberzionist ok objection sustained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839998</th>\n",
       "      <td>1</td>\n",
       "      <td>twitin baby girl sleps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839999</th>\n",
       "      <td>1</td>\n",
       "      <td>facebok u share load to myspace se promotional...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        status                                              tweet\n",
       "20000        0  heather god thing find none one like come size...\n",
       "20001        0                                           dea fun \n",
       "20002        0  tomcfly hey guy back england wel super great l...\n",
       "20003        0  gogled remedy jonzing reading one sugestions t...\n",
       "20004        0                             trying fix phone fail \n",
       "...        ...                                                ...\n",
       "839995       1  oficialy done driver ed stuf wait june license...\n",
       "839996       1      jimhunt thank sharing caring glad like quote \n",
       "839997       1               buberzionist ok objection sustained \n",
       "839998       1                            twitin baby girl sleps \n",
       "839999       1  facebok u share load to myspace se promotional...\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "    return st\n",
    "\n",
    "df2['tweet'] = df2.tweet.apply(lemmatize_text)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the average length of the words in a tweet and the polarity of the selected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of each tweet :  7.718925\n",
      "Percentage of reviews with positive sentiment is 50.0%\n",
      "Percentage of reviews with negative sentiment is 50.0%\n"
     ]
    }
   ],
   "source": [
    "s = 0.0\n",
    "for i in df2['tweet']:\n",
    "    word_list = i.split()\n",
    "    s = s + len(word_list)\n",
    "print(\"Average length of each tweet : \",s/df2.shape[0])\n",
    "pos = 0\n",
    "for i in range(df2.shape[0]):\n",
    "    if df2.iloc[i]['status'] == 1:\n",
    "        pos = pos + 1\n",
    "neg = df2.shape[0]-pos\n",
    "print(\"Percentage of reviews with positive sentiment is \"+str(pos/df2.shape[0]*100)+\"%\")\n",
    "print(\"Percentage of reviews with negative sentiment is \"+str(neg/df2.shape[0]*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the feature and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df2['tweet'].values\n",
    "labels = df2['status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tweets, labels, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4000 \n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = sequence.pad_sequences(train_sequences, padding = 'post', maxlen = max_length)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = sequence.pad_sequences(test_sequences, padding = 'post', maxlen = max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using Sequential model from LSTM, One of the natural language processing tools too create  the clasification model to oredict the polarity of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 100)          400000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 487601 (1.86 MB)\n",
      "Trainable params: 487601 (1.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(24, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During each epoch, the model updates its parameters (weights and biases) based on the gradients of the loss function with respect to those parameters. The number of epochs is a hyperparameter that is typically set before training begins and can be adjusted based on the performance of the model on a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\PRAKASH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\PRAKASH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "844/844 [==============================] - 92s 99ms/step - loss: 0.5307 - accuracy: 0.7276 - val_loss: 0.4987 - val_accuracy: 0.7573\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 85s 101ms/step - loss: 0.4348 - accuracy: 0.7999 - val_loss: 0.5080 - val_accuracy: 0.7583\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 86s 101ms/step - loss: 0.3874 - accuracy: 0.8267 - val_loss: 0.5215 - val_accuracy: 0.7560\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 83s 98ms/step - loss: 0.3367 - accuracy: 0.8489 - val_loss: 0.5954 - val_accuracy: 0.7467\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 77s 91ms/step - loss: 0.2900 - accuracy: 0.8714 - val_loss: 0.6565 - val_accuracy: 0.7503\n",
      "Epoch 6/10\n",
      "844/844 [==============================] - 78s 92ms/step - loss: 0.2486 - accuracy: 0.8899 - val_loss: 0.7911 - val_accuracy: 0.7447\n",
      "Epoch 7/10\n",
      "844/844 [==============================] - 74s 88ms/step - loss: 0.2165 - accuracy: 0.9044 - val_loss: 0.8465 - val_accuracy: 0.7270\n",
      "Epoch 8/10\n",
      "844/844 [==============================] - 73s 86ms/step - loss: 0.1848 - accuracy: 0.9190 - val_loss: 0.9962 - val_accuracy: 0.7277\n",
      "Epoch 9/10\n",
      "844/844 [==============================] - 73s 86ms/step - loss: 0.1590 - accuracy: 0.9299 - val_loss: 1.1564 - val_accuracy: 0.7183\n",
      "Epoch 10/10\n",
      "844/844 [==============================] - 75s 89ms/step - loss: 0.1389 - accuracy: 0.9393 - val_loss: 1.2793 - val_accuracy: 0.7230\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(train_padded, Y_train, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 12s 35ms/step\n",
      "Accuracy of prediction on test set :  0.7233\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(Y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2093 - accuracy: 0.7233\n",
      "Accuracy : 0.7232999801635742\n"
     ]
    }
   ],
   "source": [
    "ypred = model.evaluate(test_padded, Y_test)\n",
    "\n",
    "print(f\"Accuracy :\",ypred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model using the random sentenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "The movie was very touching and heart whelming\n",
      "Predicted sentiment :  Positive\n",
      "I have never seen a terrible movie like this\n",
      "Predicted sentiment :  Negative\n",
      "the acting was amazing and refreshing\n",
      "Predicted sentiment :  Positive\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen a terrible movie like this\", \n",
    "            \"the acting was amazing and refreshing\"]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "padded = sequence.pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "prediction = model.predict(padded)\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "for i in range(len(sentence)):\n",
    "    print(sentence[i])\n",
    "    if pred_labels[i] == 1:\n",
    "        s = 'Positive'\n",
    "    else:\n",
    "        s = 'Negative'\n",
    "    print(\"Predicted sentiment : \",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savinng and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'D:\\datascience\\twitter_guvi_project\\Twitter_Sentiment_Analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 23 variables whereas the saved optimizer has 1 variables. \n"
     ]
    }
   ],
   "source": [
    "with open(r'D:\\datascience\\twitter_guvi_project\\Twitter_Sentiment_Analysis.pkl', 'rb') as f:\n",
    "    tw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "The movie was very touching and heart whelming\n",
      "Predicted sentiment :  Positive\n",
      "I have never seen a terrible movie like this\n",
      "Predicted sentiment :  Negative\n",
      "the acting was amazing and refreshing\n",
      "Predicted sentiment :  Positive\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"The movie was very touching and heart whelming\", \n",
    "            \"I have never seen a terrible movie like this\", \n",
    "            \"the acting was amazing and refreshing\"]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "padded = sequence.pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "prediction = tw.predict(padded)\n",
    "pred_labels = []\n",
    "for i in prediction:\n",
    "    if i >= 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "for i in range(len(sentence)):\n",
    "    print(sentence[i])\n",
    "    if pred_labels[i] == 1:\n",
    "        s = 'Positive'\n",
    "    else:\n",
    "        s = 'Negative'\n",
    "    print(\"Predicted sentiment : \",s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
